---
title: "super learner"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, error = FALSE)
library(SuperLearner)
library(rpart)      # CART
library(xgboost)    # XGBoost
# library(randomForest) # random forest
library(ranger)
```

# Background

SuperLearner is an algorithm that uses cross-validation to estimate the performance of multiple machine learning models, 
or the same model with different settings. It then creates an optimal weighted average of those models, aka an “ensemble”, 
using the test data performance. This approach has been proven to be asymptotically as accurate as the best possible prediction 
algorithm that is tested.

# Software requirements and installation

# Setup dataset
```{r}
############################
# Setup example dataset.

# Load a dataset from the MASS package.
data(Boston, package = "MASS")

# Review info on the Boston dataset.
?MASS::Boston

# Check for any missing data - looks like we don't have any.
colSums(is.na(Boston))
```

```{r}
# Extract our outcome variable from the dataframe.
outcome = Boston$medv

# Create a dataframe to contain our explanatory variables.
data = subset(Boston, select = -medv)

# Check structure of our dataframe.
str(data)
```

```{r}
# If we had factor variables we would use model.matrix() to convert to numerics.

# Review our dimensions.
dim(data)
```

```{r}
# Set a seed for reproducibility in this random sampling.
set.seed(1)

# Reduce to a dataset of 150 observations to speed up model fitting.
train_obs = sample(nrow(data), 150)

# X is our training sample.
x_train = data[train_obs, ]

# Create a holdout set for evaluating model performance.
# Note: cross-validation is even better than a single holdout sample.
x_holdout = data[-train_obs, ]

# Create a binary outcome variable: towns in which median home value is > 22,000.
outcome_bin = as.numeric(outcome > 22)

y_train = outcome_bin[train_obs]
y_holdout = outcome_bin[-train_obs]

# Review the outcome variable distribution.
table(y_train, useNA = "ifany")
```

# Review available models

```{r}
# Review available models.
listWrappers()
```

```{r}
# Peek at code for a model.
SL.glmnet
```

For maximum accuracy one might try at least the following models: glmnet, randomForest, XGBoost, SVM, and bartMachine. 

These should ideally be tested with multiple hyperparameter settings for each algorithm.

# Fit individual models

Let’s fit 2 separate models: lasso (sparse, penalized OLS) and random forest. We specify family = binomial() because we are 
predicting a binary outcome, aka classification. With a continuous outcome we would specify family = gaussian().

```{r}
# Set the seed for reproducibility.
set.seed(1)

# Fit lasso model.
sl_lasso = SuperLearner(Y = y_train, X = x_train, family = binomial(),
                        SL.library = "SL.glmnet")
```

```{r}
sl_lasso
```

```{r}
# Review the elements in the SuperLearner object.
names(sl_lasso)
```

```{r}
# Here is the risk of the best model (discrete SuperLearner winner).
sl_lasso$cvRisk[which.min(sl_lasso$cvRisk)]
```

```{r}
# Here is the raw glmnet result object:
str(sl_lasso$fitLibrary$SL.glmnet_All$object, max.level = 1)
```

```{r}
# Fit random forest.
sl_rf = SuperLearner(Y = y_train, X = x_train, family = binomial(),
                     SL.library = "SL.ranger")
sl_rf
```

Risk is a measure of model accuracy or performance. We want our models to minimize the estimated risk, 
which means the model is making the fewest mistakes in its prediction. It’s basically the mean-squared error 
in a regression model, but you can customize it if you want.

SuperLearner is using cross-validation to estimate the risk on future data. By default it uses 10 folds; use 
the cvControl argument to customize.

The coefficient column tells us the weight or importance of each individual learner in the overall ensemble. 
By default the weights are always greater than or equal to 0 and sum to 1. In this case we only have one algorithm 
so the coefficient has to be 1. If a coefficient is 0 it means that the algorithm isn’t being used in the 
SuperLearner ensemble.

# Fit multiple models

Instead of fitting the models separately and looking at the performance (lowest risk), we can fit them simultaneously. 
SuperLearner will then tell us which one is best (discrete winner) and also create a weighted average of multiple models.

```{r}
set.seed(1)
sl = SuperLearner(Y = y_train, X = x_train, family = binomial(),
  SL.library = c("SL.mean", "SL.glmnet", "SL.ranger"))
sl
```

```{r}
# Review how long it took to run the SuperLearner:
sl$times$everything
```

# Predict on new data

```{r}
# Predict back on the holdout dataset.
# onlySL is set to TRUE so we don't fit algorithms that had weight = 0, saving computation.
pred = predict(sl, x_holdout, onlySL = TRUE)

# Check the structure of this prediction object.
str(pred)
```

```{r}
# Review the columns of $library.predict.
summary(pred$library.predict)
```

```{r}
# Histogram of our predicted values.
library(ggplot2)
qplot(pred$pred[, 1]) + theme_classic()
```

```{r}
# Scatterplot of original values (0, 1) and predicted values.
# Ideally we would use jitter or slight transparency to deal with overlap.
qplot(y_holdout, pred$pred[, 1]) + theme_minimal()
```

```{r}
# Review AUC - Area Under Curve
pred_rocr = ROCR::prediction(pred$pred, y_holdout)
auc = ROCR::performance(pred_rocr, measure = "auc", x.measure = "cutoff")@y.values[[1]]
auc
```

# Fit ensemble with external cross-validation

```{r}
set.seed(1)

# Don't have timing info for the CV.SuperLearner unfortunately.
# So we need to time it manually.

system.time({
  # This will take about 2x as long as the previous SuperLearner.
  cv_sl = CV.SuperLearner(Y = y_train, X = x_train, family = binomial(),
                          # For a real analysis we would use V = 10.
                          V = 3,
                          SL.library = c("SL.mean", "SL.glmnet", "SL.ranger"))
})
```

```{r}
# We run summary on the cv_sl object rather than simply printing the object.
summary(cv_sl)

cv_sl$libraryNames      
cv_sl$coef  
```

```{r}
# Review the distribution of the best single learner as external CV folds.
table(simplify2array(cv_sl$whichDiscreteSL))
```

```{r}
# Plot the performance with 95% CIs (use a better ggplot theme).
plot(cv_sl) + theme_bw()
```

```{r}
# Save plot to a file.
# ggsave("SuperLearner.png")
```

# Customize a model hyperparameter

```{r}
# Review the function argument defaults at the top.
SL.ranger
```

```{r}
# Create a new function that changes just the ntree argument.
# (We could do this in a single line.)
# "..." means "all other arguments that were sent to the function"
SL.rf.better = function(...) {
  SL.randomForest(..., num.trees = 1000)
}

set.seed(1)

# Fit the CV.SuperLearner.
# We use V = 3 to save computation time; for a real analysis use V = 10 or 20.
cv_sl = CV.SuperLearner(Y = y_train, X = x_train, family = binomial(), V = 3,
                        SL.library = c("SL.mean", "SL.glmnet", "SL.rf.better", "SL.ranger"))
```

```{r}
# Review results.
summary(cv_sl)
```

```{r}
# Customize the defaults for random forest.
learners = create.Learner("SL.ranger", params = list(num.trees = 1000))

# Look at the object.
learners
```

```{r}
# List the functions that were created
learners$names
```

```{r}
# Review the code that was automatically generated for the function.
# Notice that it's exactly the same as the function we made manually.
SL.ranger_1
```

```{r}
set.seed(1)

# Fit the CV.SuperLearner.
# We use V = 3 to save computation time; for a real analysis use V = 10 or 20.
cv_sl = CV.SuperLearner(Y = y_train, X = x_train, family = binomial(),
                        V = 3,
                        SL.library = c("SL.mean", "SL.glmnet", learners$names, "SL.ranger"))

# Review results.
summary(cv_sl)

# get superlearner
cv_sl
```

# Test algorithm with multiple hyperparameter settings

```{r}
# sqrt(p) is the default value of mtry for classification.
floor(sqrt(ncol(x_train)))
```

```{r}
# Let's try 3 multiplies of this default: 0.5, 1, and 2.
(mtry_seq = floor(sqrt(ncol(x_train)) * c(0.5, 1, 2)))
```

```{r}
learners = create.Learner("SL.ranger", tune = list(mtry = mtry_seq))

# Review the resulting object
learners
```

```{r}
# Check code for the learners that were created.
SL.ranger_1
SL.ranger_2
SL.ranger_3
```

```{r}
set.seed(1)

# Fit the CV.SuperLearner.
# We use V = 3 to save computation time; for a real analysis use V = 10 or 20.
cv_sl = CV.SuperLearner(Y = y_train, X = x_train, family = binomial(), V = 3,
                        SL.library = c("SL.mean", "SL.glmnet", learners$names, "SL.ranger"))

# Review results.
summary(cv_sl)
```

# Multicore parallelization

```{r}
# Setup parallel computation - use all cores on our computer.
(num_cores = RhpcBLASctl::get_num_cores())
```

```{r}
# Use 2 of those cores for parallel SuperLearner.
# Replace "2" with "num_cores" (without quotes) to use all cores.
options(mc.cores = num_cores)

# Check how many parallel workers we are using (on macOS/Linux).
getOption("mc.cores")
```

```{r}
# We need to set a different type of seed that works across cores.
# Otherwise the other cores will go rogue and we won't get repeatable results.
# This version is for the "multicore" parallel system in R.
set.seed(1, "L'Ecuyer-CMRG")

# While this is running check CPU using in Activity Monitor / Task Manager.
system.time({
  cv_sl = CV.SuperLearner(Y = y_train, X = x_train, family = binomial(),
                          # For a real analysis we would use V = 10.
                          V = 3,
                          parallel = "multicore",
                          SL.library = c("SL.mean", "SL.glmnet", learners$names, "SL.ranger"))
})
```

```{r}
# Review results.
summary(cv_sl)
```

```{r}
# Set multicore compatible seed.
set.seed(1, "L'Ecuyer-CMRG")

# Fit the SuperLearner.
(sl = mcSuperLearner(Y = y_train, X = x_train, family = binomial(),
                    SL.library = c("SL.mean", "SL.glmnet", learners$names, "SL.ranger")))
```

```{r}
# We see the time is reduced over our initial single-core superlearner.
sl$times$everything
```

# Weight distribution for SuperLearner

```{r}
# Review meta-weights (coefficients) from a CV.SuperLearner object
review_weights = function(cv_sl) {
  meta_weights = coef(cv_sl)
  means = colMeans(meta_weights)
  sds = apply(meta_weights, MARGIN = 2,  FUN = sd)
  mins = apply(meta_weights, MARGIN = 2, FUN = min)
  maxs = apply(meta_weights, MARGIN = 2, FUN = max)
  # Combine the stats into a single matrix.
  sl_stats = cbind("mean(weight)" = means, "sd" = sds, "min" = mins, "max" = maxs)
  # Sort by decreasing mean weight.
  sl_stats[order(sl_stats[, 1], decreasing = TRUE), ]
}

print(review_weights(cv_sl), digits = 3)
```

# Feature selection (screening)

```{r}
listWrappers()
```

```{r}
screen.corP
```

```{r}
set.seed(1)

# Fit the SuperLearner.
# We need to use list() instead of c(). cause we combine "SL.glmnet" and "screen.corP" in a vector.
cv_sl = CV.SuperLearner(Y = y_train, X = x_train, family = binomial(),
                        # For a real analysis we would use V = 10.
                        V = 3,
                        parallel = "multicore",
                        SL.library = list("SL.mean", "SL.glmnet", c("SL.glmnet", "screen.corP")))
summary(cv_sl)
```

# Optimize for AUC

```{r}
set.seed(1)

cv_sl = CV.SuperLearner(Y = y_train, X = x_train, family = binomial(),
                        # For a real analysis we would use V = 10.
                        V = 3,
                        method = "method.AUC",
                        SL.library = list("SL.mean", "SL.glmnet", c("SL.glmnet", "screen.corP")))
```

```{r}
summary(cv_sl)
```

```{r}
set.seed(1)

cv_sl = CV.SuperLearner(Y = y_train, X = x_train, family = binomial(),
                        # For a real analysis we would use V = 10.
                        V = 3,
                        method = "method.NNloglik",
                        SL.library = list("SL.mean", "SL.glmnet", c("SL.glmnet", "screen.corP")))
```

```{r}
summary(cv_sl)
```

# XGBoost hyperparameter exploration

```{r}
# 3 * 3 * 3 = 27 different configurations.
# For a real analysis we would do 100, 500, or 1000 trees - this is just a demo.
tune = list(ntrees = c(10, 20, 50),
            max_depth = 1:3,
            shrinkage = c(0.001, 0.01, 0.1))

# Set detailed names = T so we can see the configuration for each function.
# Also shorten the name prefix.
learners = create.Learner("SL.xgboost", tune = tune, detailed_names = TRUE, name_prefix = "xgb")

# 27 configurations - not too shabby.
length(learners$names)
```

```{r}
learners$names
```

```{r}
# Confirm we have multiple cores configured. This should be > 1.
getOption("mc.cores")
```

```{r}
# Remember to set multicore-compatible seed.
set.seed(1, "L'Ecuyer-CMRG")

# Fit the CV.SuperLearner.
system.time({
  cv_sl = CV.SuperLearner(Y = y_train, X = x_train, family = binomial(),
                          # For a real analysis we would use V = 10.
                          V = 3,
                          parallel = "multicore",
                          SL.library = c("SL.mean", "SL.glmnet", learners$names, "SL.ranger"))
})
```

```{r}
# Review results.
summary(cv_sl)
```

```{r}
review_weights(cv_sl)
```

```{r}
plot(cv_sl) + theme_bw()
```